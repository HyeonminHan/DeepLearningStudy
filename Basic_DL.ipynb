{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "weights = torch.randn(256, 4) / math.sqrt(256) \n",
    "# print(weights.shape)\n",
    "\n",
    "## 256x4 행렬의 숫자가 경사 역전파를 통해 조정될 수 있게 만든다.\n",
    "weights.requires_grad_()\n",
    "\n",
    "## 편향값 훈련 가능하게 설정\n",
    "bias = torch.zeros(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 여기서 부터 실습\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module) : \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1) ## 입력 1채널, 출력 16채널, 커널 3, 스트라이드 1\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 = 12x12x32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) ##Negative Log Likelihood \n",
    "        loss.backward()\n",
    "        optim.step() ##최적화 과정\n",
    "        if b_i % 10 == 0 :\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)] \\t training loss : {:.6f}'.format(epoch, b_i, len(train_dataloader), b_i / len(train_dataloader)*100, loss.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            \n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()\n",
    "            \n",
    "            ##가장 킹능성이 높은 예측을 얻기 위해 argmax 사용함\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss /= len(test_dataloader.dataset)\n",
    "        print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:0f}%)\\n'.format(loss, success, len(test_dataloader.dataset), 100. * success / len(test_dataloader.dataset)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), \\\n",
    "                                                                                                                               transforms.Normalize((0.1302,), (0.3069))])),\\\n",
    "                                             batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, transform = transforms.Compose([transforms.ToTensor(),\\\n",
    "                                                                                                                   transforms.Normalize((0.1302,), (0.3069,))\\\n",
    "                                                                                                                   ])),\\\n",
    "                                             batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/1875 (0%)] \t training loss : 0.163313\n",
      "epoch: 1 [10/1875 (1%)] \t training loss : 0.137107\n",
      "epoch: 1 [20/1875 (1%)] \t training loss : 0.064922\n",
      "epoch: 1 [30/1875 (2%)] \t training loss : 0.027744\n",
      "epoch: 1 [40/1875 (2%)] \t training loss : 0.180184\n",
      "epoch: 1 [50/1875 (3%)] \t training loss : 0.017911\n",
      "epoch: 1 [60/1875 (3%)] \t training loss : 0.002331\n",
      "epoch: 1 [70/1875 (4%)] \t training loss : 0.077977\n",
      "epoch: 1 [80/1875 (4%)] \t training loss : 0.008578\n",
      "epoch: 1 [90/1875 (5%)] \t training loss : 0.045218\n",
      "epoch: 1 [100/1875 (5%)] \t training loss : 0.040113\n",
      "epoch: 1 [110/1875 (6%)] \t training loss : 0.049325\n",
      "epoch: 1 [120/1875 (6%)] \t training loss : 0.048317\n",
      "epoch: 1 [130/1875 (7%)] \t training loss : 0.188791\n",
      "epoch: 1 [140/1875 (7%)] \t training loss : 0.011541\n",
      "epoch: 1 [150/1875 (8%)] \t training loss : 0.069988\n",
      "epoch: 1 [160/1875 (9%)] \t training loss : 0.031489\n",
      "epoch: 1 [170/1875 (9%)] \t training loss : 0.021596\n",
      "epoch: 1 [180/1875 (10%)] \t training loss : 0.045939\n",
      "epoch: 1 [190/1875 (10%)] \t training loss : 0.008523\n",
      "epoch: 1 [200/1875 (11%)] \t training loss : 0.006919\n",
      "epoch: 1 [210/1875 (11%)] \t training loss : 0.011325\n",
      "epoch: 1 [220/1875 (12%)] \t training loss : 0.112257\n",
      "epoch: 1 [230/1875 (12%)] \t training loss : 0.003351\n",
      "epoch: 1 [240/1875 (13%)] \t training loss : 0.015408\n",
      "epoch: 1 [250/1875 (13%)] \t training loss : 0.003444\n",
      "epoch: 1 [260/1875 (14%)] \t training loss : 0.090242\n",
      "epoch: 1 [270/1875 (14%)] \t training loss : 0.049011\n",
      "epoch: 1 [280/1875 (15%)] \t training loss : 0.083492\n",
      "epoch: 1 [290/1875 (15%)] \t training loss : 0.014528\n",
      "epoch: 1 [300/1875 (16%)] \t training loss : 0.028169\n",
      "epoch: 1 [310/1875 (17%)] \t training loss : 0.114630\n",
      "epoch: 1 [320/1875 (17%)] \t training loss : 0.109183\n",
      "epoch: 1 [330/1875 (18%)] \t training loss : 0.122420\n",
      "epoch: 1 [340/1875 (18%)] \t training loss : 0.072213\n",
      "epoch: 1 [350/1875 (19%)] \t training loss : 0.133751\n",
      "epoch: 1 [360/1875 (19%)] \t training loss : 0.010323\n",
      "epoch: 1 [370/1875 (20%)] \t training loss : 0.040496\n",
      "epoch: 1 [380/1875 (20%)] \t training loss : 0.004405\n",
      "epoch: 1 [390/1875 (21%)] \t training loss : 0.007917\n",
      "epoch: 1 [400/1875 (21%)] \t training loss : 0.012952\n",
      "epoch: 1 [410/1875 (22%)] \t training loss : 0.111404\n",
      "epoch: 1 [420/1875 (22%)] \t training loss : 0.015660\n",
      "epoch: 1 [430/1875 (23%)] \t training loss : 0.098714\n",
      "epoch: 1 [440/1875 (23%)] \t training loss : 0.025990\n",
      "epoch: 1 [450/1875 (24%)] \t training loss : 0.025061\n",
      "epoch: 1 [460/1875 (25%)] \t training loss : 0.015995\n",
      "epoch: 1 [470/1875 (25%)] \t training loss : 0.110179\n",
      "epoch: 1 [480/1875 (26%)] \t training loss : 0.091707\n",
      "epoch: 1 [490/1875 (26%)] \t training loss : 0.015036\n",
      "epoch: 1 [500/1875 (27%)] \t training loss : 0.168110\n",
      "epoch: 1 [510/1875 (27%)] \t training loss : 0.128523\n",
      "epoch: 1 [520/1875 (28%)] \t training loss : 0.063280\n",
      "epoch: 1 [530/1875 (28%)] \t training loss : 0.004341\n",
      "epoch: 1 [540/1875 (29%)] \t training loss : 0.113277\n",
      "epoch: 1 [550/1875 (29%)] \t training loss : 0.028051\n",
      "epoch: 1 [560/1875 (30%)] \t training loss : 0.029023\n",
      "epoch: 1 [570/1875 (30%)] \t training loss : 0.211190\n",
      "epoch: 1 [580/1875 (31%)] \t training loss : 0.009062\n",
      "epoch: 1 [590/1875 (31%)] \t training loss : 0.028116\n",
      "epoch: 1 [600/1875 (32%)] \t training loss : 0.061777\n",
      "epoch: 1 [610/1875 (33%)] \t training loss : 0.058920\n",
      "epoch: 1 [620/1875 (33%)] \t training loss : 0.009901\n",
      "epoch: 1 [630/1875 (34%)] \t training loss : 0.115753\n",
      "epoch: 1 [640/1875 (34%)] \t training loss : 0.004296\n",
      "epoch: 1 [650/1875 (35%)] \t training loss : 0.432131\n",
      "epoch: 1 [660/1875 (35%)] \t training loss : 0.047699\n",
      "epoch: 1 [670/1875 (36%)] \t training loss : 0.207646\n",
      "epoch: 1 [680/1875 (36%)] \t training loss : 0.213866\n",
      "epoch: 1 [690/1875 (37%)] \t training loss : 0.007297\n",
      "epoch: 1 [700/1875 (37%)] \t training loss : 0.005726\n",
      "epoch: 1 [710/1875 (38%)] \t training loss : 0.003312\n",
      "epoch: 1 [720/1875 (38%)] \t training loss : 0.003066\n",
      "epoch: 1 [730/1875 (39%)] \t training loss : 0.071552\n",
      "epoch: 1 [740/1875 (39%)] \t training loss : 0.035335\n",
      "epoch: 1 [750/1875 (40%)] \t training loss : 0.018302\n",
      "epoch: 1 [760/1875 (41%)] \t training loss : 0.127022\n",
      "epoch: 1 [770/1875 (41%)] \t training loss : 0.021220\n",
      "epoch: 1 [780/1875 (42%)] \t training loss : 0.215231\n",
      "epoch: 1 [790/1875 (42%)] \t training loss : 0.038017\n",
      "epoch: 1 [800/1875 (43%)] \t training loss : 0.026488\n",
      "epoch: 1 [810/1875 (43%)] \t training loss : 0.007510\n",
      "epoch: 1 [820/1875 (44%)] \t training loss : 0.022054\n",
      "epoch: 1 [830/1875 (44%)] \t training loss : 0.143065\n",
      "epoch: 1 [840/1875 (45%)] \t training loss : 0.012487\n",
      "epoch: 1 [850/1875 (45%)] \t training loss : 0.245200\n",
      "epoch: 1 [860/1875 (46%)] \t training loss : 0.010808\n",
      "epoch: 1 [870/1875 (46%)] \t training loss : 0.077884\n",
      "epoch: 1 [880/1875 (47%)] \t training loss : 0.237930\n",
      "epoch: 1 [890/1875 (47%)] \t training loss : 0.102017\n",
      "epoch: 1 [900/1875 (48%)] \t training loss : 0.012011\n",
      "epoch: 1 [910/1875 (49%)] \t training loss : 0.044792\n",
      "epoch: 1 [920/1875 (49%)] \t training loss : 0.020367\n",
      "epoch: 1 [930/1875 (50%)] \t training loss : 0.008347\n",
      "epoch: 1 [940/1875 (50%)] \t training loss : 0.036475\n",
      "epoch: 1 [950/1875 (51%)] \t training loss : 0.063444\n",
      "epoch: 1 [960/1875 (51%)] \t training loss : 0.095159\n",
      "epoch: 1 [970/1875 (52%)] \t training loss : 0.015026\n",
      "epoch: 1 [980/1875 (52%)] \t training loss : 0.001479\n",
      "epoch: 1 [990/1875 (53%)] \t training loss : 0.026118\n",
      "epoch: 1 [1000/1875 (53%)] \t training loss : 0.019155\n",
      "epoch: 1 [1010/1875 (54%)] \t training loss : 0.209801\n",
      "epoch: 1 [1020/1875 (54%)] \t training loss : 0.138460\n",
      "epoch: 1 [1030/1875 (55%)] \t training loss : 0.213551\n",
      "epoch: 1 [1040/1875 (55%)] \t training loss : 0.065481\n",
      "epoch: 1 [1050/1875 (56%)] \t training loss : 0.193473\n",
      "epoch: 1 [1060/1875 (57%)] \t training loss : 0.121544\n",
      "epoch: 1 [1070/1875 (57%)] \t training loss : 0.154299\n",
      "epoch: 1 [1080/1875 (58%)] \t training loss : 0.092907\n",
      "epoch: 1 [1090/1875 (58%)] \t training loss : 0.011922\n",
      "epoch: 1 [1100/1875 (59%)] \t training loss : 0.004025\n",
      "epoch: 1 [1110/1875 (59%)] \t training loss : 0.067673\n",
      "epoch: 1 [1120/1875 (60%)] \t training loss : 0.003711\n",
      "epoch: 1 [1130/1875 (60%)] \t training loss : 0.043669\n",
      "epoch: 1 [1140/1875 (61%)] \t training loss : 0.226718\n",
      "epoch: 1 [1150/1875 (61%)] \t training loss : 0.079153\n",
      "epoch: 1 [1160/1875 (62%)] \t training loss : 0.399014\n",
      "epoch: 1 [1170/1875 (62%)] \t training loss : 0.032679\n",
      "epoch: 1 [1180/1875 (63%)] \t training loss : 0.286396\n",
      "epoch: 1 [1190/1875 (63%)] \t training loss : 0.106179\n",
      "epoch: 1 [1200/1875 (64%)] \t training loss : 0.061099\n",
      "epoch: 1 [1210/1875 (65%)] \t training loss : 0.063676\n",
      "epoch: 1 [1220/1875 (65%)] \t training loss : 0.061343\n",
      "epoch: 1 [1230/1875 (66%)] \t training loss : 0.012584\n",
      "epoch: 1 [1240/1875 (66%)] \t training loss : 0.001307\n",
      "epoch: 1 [1250/1875 (67%)] \t training loss : 0.212612\n",
      "epoch: 1 [1260/1875 (67%)] \t training loss : 0.017790\n",
      "epoch: 1 [1270/1875 (68%)] \t training loss : 0.005642\n",
      "epoch: 1 [1280/1875 (68%)] \t training loss : 0.237311\n",
      "epoch: 1 [1290/1875 (69%)] \t training loss : 0.137379\n",
      "epoch: 1 [1300/1875 (69%)] \t training loss : 0.097286\n",
      "epoch: 1 [1310/1875 (70%)] \t training loss : 0.137006\n",
      "epoch: 1 [1320/1875 (70%)] \t training loss : 0.001382\n",
      "epoch: 1 [1330/1875 (71%)] \t training loss : 0.007340\n",
      "epoch: 1 [1340/1875 (71%)] \t training loss : 0.114668\n",
      "epoch: 1 [1350/1875 (72%)] \t training loss : 0.004262\n",
      "epoch: 1 [1360/1875 (73%)] \t training loss : 0.069135\n",
      "epoch: 1 [1370/1875 (73%)] \t training loss : 0.556599\n",
      "epoch: 1 [1380/1875 (74%)] \t training loss : 0.006275\n",
      "epoch: 1 [1390/1875 (74%)] \t training loss : 0.046251\n",
      "epoch: 1 [1400/1875 (75%)] \t training loss : 0.064987\n",
      "epoch: 1 [1410/1875 (75%)] \t training loss : 0.062364\n",
      "epoch: 1 [1420/1875 (76%)] \t training loss : 0.064551\n",
      "epoch: 1 [1430/1875 (76%)] \t training loss : 0.033573\n",
      "epoch: 1 [1440/1875 (77%)] \t training loss : 0.031194\n",
      "epoch: 1 [1450/1875 (77%)] \t training loss : 0.015657\n",
      "epoch: 1 [1460/1875 (78%)] \t training loss : 0.021902\n",
      "epoch: 1 [1470/1875 (78%)] \t training loss : 0.003497\n",
      "epoch: 1 [1480/1875 (79%)] \t training loss : 0.013811\n",
      "epoch: 1 [1490/1875 (79%)] \t training loss : 0.009992\n",
      "epoch: 1 [1500/1875 (80%)] \t training loss : 0.018562\n",
      "epoch: 1 [1510/1875 (81%)] \t training loss : 0.000397\n",
      "epoch: 1 [1520/1875 (81%)] \t training loss : 0.107778\n",
      "epoch: 1 [1530/1875 (82%)] \t training loss : 0.080499\n",
      "epoch: 1 [1540/1875 (82%)] \t training loss : 0.005518\n",
      "epoch: 1 [1550/1875 (83%)] \t training loss : 0.022483\n",
      "epoch: 1 [1560/1875 (83%)] \t training loss : 0.273554\n",
      "epoch: 1 [1570/1875 (84%)] \t training loss : 0.011920\n",
      "epoch: 1 [1580/1875 (84%)] \t training loss : 0.005327\n",
      "epoch: 1 [1590/1875 (85%)] \t training loss : 0.027366\n",
      "epoch: 1 [1600/1875 (85%)] \t training loss : 0.003285\n",
      "epoch: 1 [1610/1875 (86%)] \t training loss : 0.007790\n",
      "epoch: 1 [1620/1875 (86%)] \t training loss : 0.033294\n",
      "epoch: 1 [1630/1875 (87%)] \t training loss : 0.011370\n",
      "epoch: 1 [1640/1875 (87%)] \t training loss : 0.057780\n",
      "epoch: 1 [1650/1875 (88%)] \t training loss : 0.019720\n",
      "epoch: 1 [1660/1875 (89%)] \t training loss : 0.050232\n",
      "epoch: 1 [1670/1875 (89%)] \t training loss : 0.032809\n",
      "epoch: 1 [1680/1875 (90%)] \t training loss : 0.051879\n",
      "epoch: 1 [1690/1875 (90%)] \t training loss : 0.123969\n",
      "epoch: 1 [1700/1875 (91%)] \t training loss : 0.130625\n",
      "epoch: 1 [1710/1875 (91%)] \t training loss : 0.110450\n",
      "epoch: 1 [1720/1875 (92%)] \t training loss : 0.007272\n",
      "epoch: 1 [1730/1875 (92%)] \t training loss : 0.279256\n",
      "epoch: 1 [1740/1875 (93%)] \t training loss : 0.077180\n",
      "epoch: 1 [1750/1875 (93%)] \t training loss : 0.022457\n",
      "epoch: 1 [1760/1875 (94%)] \t training loss : 0.041007\n",
      "epoch: 1 [1770/1875 (94%)] \t training loss : 0.082403\n",
      "epoch: 1 [1780/1875 (95%)] \t training loss : 0.251936\n",
      "epoch: 1 [1790/1875 (95%)] \t training loss : 0.170784\n",
      "epoch: 1 [1800/1875 (96%)] \t training loss : 0.004917\n",
      "epoch: 1 [1810/1875 (97%)] \t training loss : 0.011019\n",
      "epoch: 1 [1820/1875 (97%)] \t training loss : 0.052112\n",
      "epoch: 1 [1830/1875 (98%)] \t training loss : 0.002031\n",
      "epoch: 1 [1840/1875 (98%)] \t training loss : 0.009011\n",
      "epoch: 1 [1850/1875 (99%)] \t training loss : 0.000671\n",
      "epoch: 1 [1860/1875 (99%)] \t training loss : 0.025959\n",
      "epoch: 1 [1870/1875 (100%)] \t training loss : 0.143561\n",
      "\n",
      "Test dataset: Overall Loss: 0.0354, Overall Accuracy: 9880/10000 (98.800000%)\n",
      "\n",
      "epoch: 2 [0/1875 (0%)] \t training loss : 0.032821\n",
      "epoch: 2 [10/1875 (1%)] \t training loss : 0.001643\n",
      "epoch: 2 [20/1875 (1%)] \t training loss : 0.054055\n",
      "epoch: 2 [30/1875 (2%)] \t training loss : 0.013600\n",
      "epoch: 2 [40/1875 (2%)] \t training loss : 0.054729\n",
      "epoch: 2 [50/1875 (3%)] \t training loss : 0.025907\n",
      "epoch: 2 [60/1875 (3%)] \t training loss : 0.017969\n",
      "epoch: 2 [70/1875 (4%)] \t training loss : 0.015443\n",
      "epoch: 2 [80/1875 (4%)] \t training loss : 0.030981\n",
      "epoch: 2 [90/1875 (5%)] \t training loss : 0.083248\n",
      "epoch: 2 [100/1875 (5%)] \t training loss : 0.050088\n",
      "epoch: 2 [110/1875 (6%)] \t training loss : 0.001238\n",
      "epoch: 2 [120/1875 (6%)] \t training loss : 0.021899\n",
      "epoch: 2 [130/1875 (7%)] \t training loss : 0.086710\n",
      "epoch: 2 [140/1875 (7%)] \t training loss : 0.023695\n",
      "epoch: 2 [150/1875 (8%)] \t training loss : 0.004316\n",
      "epoch: 2 [160/1875 (9%)] \t training loss : 0.008239\n",
      "epoch: 2 [170/1875 (9%)] \t training loss : 0.086846\n",
      "epoch: 2 [180/1875 (10%)] \t training loss : 0.043731\n",
      "epoch: 2 [190/1875 (10%)] \t training loss : 0.067464\n",
      "epoch: 2 [200/1875 (11%)] \t training loss : 0.096403\n",
      "epoch: 2 [210/1875 (11%)] \t training loss : 0.033288\n",
      "epoch: 2 [220/1875 (12%)] \t training loss : 0.002154\n",
      "epoch: 2 [230/1875 (12%)] \t training loss : 0.010842\n",
      "epoch: 2 [240/1875 (13%)] \t training loss : 0.007502\n",
      "epoch: 2 [250/1875 (13%)] \t training loss : 0.035736\n",
      "epoch: 2 [260/1875 (14%)] \t training loss : 0.006209\n",
      "epoch: 2 [270/1875 (14%)] \t training loss : 0.035394\n",
      "epoch: 2 [280/1875 (15%)] \t training loss : 0.070820\n",
      "epoch: 2 [290/1875 (15%)] \t training loss : 0.016296\n",
      "epoch: 2 [300/1875 (16%)] \t training loss : 0.006037\n",
      "epoch: 2 [310/1875 (17%)] \t training loss : 0.034461\n",
      "epoch: 2 [320/1875 (17%)] \t training loss : 0.035726\n",
      "epoch: 2 [330/1875 (18%)] \t training loss : 0.047638\n",
      "epoch: 2 [340/1875 (18%)] \t training loss : 0.149678\n",
      "epoch: 2 [350/1875 (19%)] \t training loss : 0.098623\n",
      "epoch: 2 [360/1875 (19%)] \t training loss : 0.589655\n",
      "epoch: 2 [370/1875 (20%)] \t training loss : 0.002843\n",
      "epoch: 2 [380/1875 (20%)] \t training loss : 0.006583\n",
      "epoch: 2 [390/1875 (21%)] \t training loss : 0.002405\n",
      "epoch: 2 [400/1875 (21%)] \t training loss : 0.000721\n",
      "epoch: 2 [410/1875 (22%)] \t training loss : 0.005039\n",
      "epoch: 2 [420/1875 (22%)] \t training loss : 0.007171\n",
      "epoch: 2 [430/1875 (23%)] \t training loss : 0.022055\n",
      "epoch: 2 [440/1875 (23%)] \t training loss : 0.257036\n",
      "epoch: 2 [450/1875 (24%)] \t training loss : 0.013104\n",
      "epoch: 2 [460/1875 (25%)] \t training loss : 0.010644\n",
      "epoch: 2 [470/1875 (25%)] \t training loss : 0.009181\n",
      "epoch: 2 [480/1875 (26%)] \t training loss : 0.007308\n",
      "epoch: 2 [490/1875 (26%)] \t training loss : 0.000293\n",
      "epoch: 2 [500/1875 (27%)] \t training loss : 0.020804\n",
      "epoch: 2 [510/1875 (27%)] \t training loss : 0.052204\n",
      "epoch: 2 [520/1875 (28%)] \t training loss : 0.005231\n",
      "epoch: 2 [530/1875 (28%)] \t training loss : 0.079611\n",
      "epoch: 2 [540/1875 (29%)] \t training loss : 0.060065\n",
      "epoch: 2 [550/1875 (29%)] \t training loss : 0.098889\n",
      "epoch: 2 [560/1875 (30%)] \t training loss : 0.028770\n",
      "epoch: 2 [570/1875 (30%)] \t training loss : 0.010129\n",
      "epoch: 2 [580/1875 (31%)] \t training loss : 0.051985\n",
      "epoch: 2 [590/1875 (31%)] \t training loss : 0.045931\n",
      "epoch: 2 [600/1875 (32%)] \t training loss : 0.001983\n",
      "epoch: 2 [610/1875 (33%)] \t training loss : 0.008419\n",
      "epoch: 2 [620/1875 (33%)] \t training loss : 0.019465\n",
      "epoch: 2 [630/1875 (34%)] \t training loss : 0.127825\n",
      "epoch: 2 [640/1875 (34%)] \t training loss : 0.006935\n",
      "epoch: 2 [650/1875 (35%)] \t training loss : 0.027693\n",
      "epoch: 2 [660/1875 (35%)] \t training loss : 0.018316\n",
      "epoch: 2 [670/1875 (36%)] \t training loss : 0.216662\n",
      "epoch: 2 [680/1875 (36%)] \t training loss : 0.008259\n",
      "epoch: 2 [690/1875 (37%)] \t training loss : 0.035548\n",
      "epoch: 2 [700/1875 (37%)] \t training loss : 0.070025\n",
      "epoch: 2 [710/1875 (38%)] \t training loss : 0.193831\n",
      "epoch: 2 [720/1875 (38%)] \t training loss : 0.016616\n",
      "epoch: 2 [730/1875 (39%)] \t training loss : 0.001240\n",
      "epoch: 2 [740/1875 (39%)] \t training loss : 0.001444\n",
      "epoch: 2 [750/1875 (40%)] \t training loss : 0.014865\n",
      "epoch: 2 [760/1875 (41%)] \t training loss : 0.014695\n",
      "epoch: 2 [770/1875 (41%)] \t training loss : 0.002718\n",
      "epoch: 2 [780/1875 (42%)] \t training loss : 0.065689\n",
      "epoch: 2 [790/1875 (42%)] \t training loss : 0.004057\n",
      "epoch: 2 [800/1875 (43%)] \t training loss : 0.004287\n",
      "epoch: 2 [810/1875 (43%)] \t training loss : 0.077976\n",
      "epoch: 2 [820/1875 (44%)] \t training loss : 0.017808\n",
      "epoch: 2 [830/1875 (44%)] \t training loss : 0.107758\n",
      "epoch: 2 [840/1875 (45%)] \t training loss : 0.005678\n",
      "epoch: 2 [850/1875 (45%)] \t training loss : 0.049043\n",
      "epoch: 2 [860/1875 (46%)] \t training loss : 0.002033\n",
      "epoch: 2 [870/1875 (46%)] \t training loss : 0.001564\n",
      "epoch: 2 [880/1875 (47%)] \t training loss : 0.238598\n",
      "epoch: 2 [890/1875 (47%)] \t training loss : 0.007404\n",
      "epoch: 2 [900/1875 (48%)] \t training loss : 0.183671\n",
      "epoch: 2 [910/1875 (49%)] \t training loss : 0.091185\n",
      "epoch: 2 [920/1875 (49%)] \t training loss : 0.004287\n",
      "epoch: 2 [930/1875 (50%)] \t training loss : 0.004780\n",
      "epoch: 2 [940/1875 (50%)] \t training loss : 0.011298\n",
      "epoch: 2 [950/1875 (51%)] \t training loss : 0.001873\n",
      "epoch: 2 [960/1875 (51%)] \t training loss : 0.079636\n",
      "epoch: 2 [970/1875 (52%)] \t training loss : 0.038853\n",
      "epoch: 2 [980/1875 (52%)] \t training loss : 0.027939\n",
      "epoch: 2 [990/1875 (53%)] \t training loss : 0.007630\n",
      "epoch: 2 [1000/1875 (53%)] \t training loss : 0.076764\n",
      "epoch: 2 [1010/1875 (54%)] \t training loss : 0.001782\n",
      "epoch: 2 [1020/1875 (54%)] \t training loss : 0.015398\n",
      "epoch: 2 [1030/1875 (55%)] \t training loss : 0.242356\n",
      "epoch: 2 [1040/1875 (55%)] \t training loss : 0.000524\n",
      "epoch: 2 [1050/1875 (56%)] \t training loss : 0.159340\n",
      "epoch: 2 [1060/1875 (57%)] \t training loss : 0.036226\n",
      "epoch: 2 [1070/1875 (57%)] \t training loss : 0.012809\n",
      "epoch: 2 [1080/1875 (58%)] \t training loss : 0.071676\n",
      "epoch: 2 [1090/1875 (58%)] \t training loss : 0.004909\n",
      "epoch: 2 [1100/1875 (59%)] \t training loss : 0.109795\n",
      "epoch: 2 [1110/1875 (59%)] \t training loss : 0.065305\n",
      "epoch: 2 [1120/1875 (60%)] \t training loss : 0.001008\n",
      "epoch: 2 [1130/1875 (60%)] \t training loss : 0.028147\n",
      "epoch: 2 [1140/1875 (61%)] \t training loss : 0.124480\n",
      "epoch: 2 [1150/1875 (61%)] \t training loss : 0.063617\n",
      "epoch: 2 [1160/1875 (62%)] \t training loss : 0.032520\n",
      "epoch: 2 [1170/1875 (62%)] \t training loss : 0.204749\n",
      "epoch: 2 [1180/1875 (63%)] \t training loss : 0.148903\n",
      "epoch: 2 [1190/1875 (63%)] \t training loss : 0.002656\n",
      "epoch: 2 [1200/1875 (64%)] \t training loss : 0.000563\n",
      "epoch: 2 [1210/1875 (65%)] \t training loss : 0.036502\n",
      "epoch: 2 [1220/1875 (65%)] \t training loss : 0.002311\n",
      "epoch: 2 [1230/1875 (66%)] \t training loss : 0.004709\n",
      "epoch: 2 [1240/1875 (66%)] \t training loss : 0.109850\n",
      "epoch: 2 [1250/1875 (67%)] \t training loss : 0.002313\n",
      "epoch: 2 [1260/1875 (67%)] \t training loss : 0.004014\n",
      "epoch: 2 [1270/1875 (68%)] \t training loss : 0.066295\n",
      "epoch: 2 [1280/1875 (68%)] \t training loss : 0.002951\n",
      "epoch: 2 [1290/1875 (69%)] \t training loss : 0.091638\n",
      "epoch: 2 [1300/1875 (69%)] \t training loss : 0.029992\n",
      "epoch: 2 [1310/1875 (70%)] \t training loss : 0.048208\n",
      "epoch: 2 [1320/1875 (70%)] \t training loss : 0.081326\n",
      "epoch: 2 [1330/1875 (71%)] \t training loss : 0.039327\n",
      "epoch: 2 [1340/1875 (71%)] \t training loss : 0.031776\n",
      "epoch: 2 [1350/1875 (72%)] \t training loss : 0.002419\n",
      "epoch: 2 [1360/1875 (73%)] \t training loss : 0.016962\n",
      "epoch: 2 [1370/1875 (73%)] \t training loss : 0.005099\n",
      "epoch: 2 [1380/1875 (74%)] \t training loss : 0.002879\n",
      "epoch: 2 [1390/1875 (74%)] \t training loss : 0.046227\n",
      "epoch: 2 [1400/1875 (75%)] \t training loss : 0.038975\n",
      "epoch: 2 [1410/1875 (75%)] \t training loss : 0.001101\n",
      "epoch: 2 [1420/1875 (76%)] \t training loss : 0.018297\n",
      "epoch: 2 [1430/1875 (76%)] \t training loss : 0.057492\n",
      "epoch: 2 [1440/1875 (77%)] \t training loss : 0.000910\n",
      "epoch: 2 [1450/1875 (77%)] \t training loss : 0.006129\n",
      "epoch: 2 [1460/1875 (78%)] \t training loss : 0.080365\n",
      "epoch: 2 [1470/1875 (78%)] \t training loss : 0.097048\n",
      "epoch: 2 [1480/1875 (79%)] \t training loss : 0.018295\n",
      "epoch: 2 [1490/1875 (79%)] \t training loss : 0.005653\n",
      "epoch: 2 [1500/1875 (80%)] \t training loss : 0.202890\n",
      "epoch: 2 [1510/1875 (81%)] \t training loss : 0.006115\n",
      "epoch: 2 [1520/1875 (81%)] \t training loss : 0.010630\n",
      "epoch: 2 [1530/1875 (82%)] \t training loss : 0.069293\n",
      "epoch: 2 [1540/1875 (82%)] \t training loss : 0.225847\n",
      "epoch: 2 [1550/1875 (83%)] \t training loss : 0.004167\n",
      "epoch: 2 [1560/1875 (83%)] \t training loss : 0.006069\n",
      "epoch: 2 [1570/1875 (84%)] \t training loss : 0.017491\n",
      "epoch: 2 [1580/1875 (84%)] \t training loss : 0.001906\n",
      "epoch: 2 [1590/1875 (85%)] \t training loss : 0.014490\n",
      "epoch: 2 [1600/1875 (85%)] \t training loss : 0.016465\n",
      "epoch: 2 [1610/1875 (86%)] \t training loss : 0.144426\n",
      "epoch: 2 [1620/1875 (86%)] \t training loss : 0.161580\n",
      "epoch: 2 [1630/1875 (87%)] \t training loss : 0.002833\n",
      "epoch: 2 [1640/1875 (87%)] \t training loss : 0.005603\n",
      "epoch: 2 [1650/1875 (88%)] \t training loss : 0.011424\n",
      "epoch: 2 [1660/1875 (89%)] \t training loss : 0.062517\n",
      "epoch: 2 [1670/1875 (89%)] \t training loss : 0.004763\n",
      "epoch: 2 [1680/1875 (90%)] \t training loss : 0.038791\n",
      "epoch: 2 [1690/1875 (90%)] \t training loss : 0.000592\n",
      "epoch: 2 [1700/1875 (91%)] \t training loss : 0.018397\n",
      "epoch: 2 [1710/1875 (91%)] \t training loss : 0.337212\n",
      "epoch: 2 [1720/1875 (92%)] \t training loss : 0.064219\n",
      "epoch: 2 [1730/1875 (92%)] \t training loss : 0.001522\n",
      "epoch: 2 [1740/1875 (93%)] \t training loss : 0.018320\n",
      "epoch: 2 [1750/1875 (93%)] \t training loss : 0.026598\n",
      "epoch: 2 [1760/1875 (94%)] \t training loss : 0.048171\n",
      "epoch: 2 [1770/1875 (94%)] \t training loss : 0.002566\n",
      "epoch: 2 [1780/1875 (95%)] \t training loss : 0.042384\n",
      "epoch: 2 [1790/1875 (95%)] \t training loss : 0.027121\n",
      "epoch: 2 [1800/1875 (96%)] \t training loss : 0.014587\n",
      "epoch: 2 [1810/1875 (97%)] \t training loss : 0.002976\n",
      "epoch: 2 [1820/1875 (97%)] \t training loss : 0.004737\n",
      "epoch: 2 [1830/1875 (98%)] \t training loss : 0.054927\n",
      "epoch: 2 [1840/1875 (98%)] \t training loss : 0.065711\n",
      "epoch: 2 [1850/1875 (99%)] \t training loss : 0.055747\n",
      "epoch: 2 [1860/1875 (99%)] \t training loss : 0.000494\n",
      "epoch: 2 [1870/1875 (100%)] \t training loss : 0.012775\n",
      "\n",
      "Test dataset: Overall Loss: 0.0365, Overall Accuracy: 9874/10000 (98.740000%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18280b5a280>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "graound truth is : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\handy\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"graound truth is : {sample_targets[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9502607ac120794b982d5ccb3066dfe72152880354e0dfbf31f715d9cfc03cca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
