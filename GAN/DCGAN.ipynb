{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.autograd import Variable \n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eps = 10\n",
    "bsize = 32 \n",
    "lrate = 0.001 \n",
    "lat_dimension = 64 ## 랜덤노이즈 벡터 길이 \n",
    "image_sz = 64 \n",
    "chnls = 1\n",
    "logging_intv = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GANGenerator, self).__init__()\n",
    "        self.inp_sz = image_sz // 4\n",
    "        self.lin = nn.Linear(lat_dimension, 128 * self.inp_sz ** 2) ## 첫번째 param - input dim / 두번째 param - output dim\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.up1 = nn.Upsample(scale_factor=2)\n",
    "        self.conv1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128, 0.8) ## 0.8 -> ? \n",
    "        self.rl1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.up2 = nn.Upsample(scale_factor=2)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.rl2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv3 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.lin(x)\n",
    "\n",
    "        x = x.view(x.shape[0], 128, self.inp_sz, self.inp_sz) ## x.shape[0] - 배치사이즈? \n",
    "        x = self.bn1(x)\n",
    "        x = self.up1(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.rl1(x)\n",
    "\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.rl2(x) \n",
    "\n",
    "        x = self.conv3(x)\n",
    "        out = self.act(x)\n",
    "\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDiscriminator(nn.module):\n",
    "    def __init__(self):\n",
    "        super(GANDiscriminator, self).__init__()\n",
    "\n",
    "        def disc_module(input_channels, output_channels, bnorm=True):\n",
    "            mod = [nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=2, padding=1), \n",
    "                   nn.LeakyReLU(0.2, inplace=True), \n",
    "                   nn.Dropout(0.25)]\n",
    "            if bnorm:\n",
    "                mod += [nn.BatchNorm2d(output_channels, 0.8)]\n",
    "            \n",
    "            return mod\n",
    "        \n",
    "        self.disc_model = nn.Sequential(\n",
    "            *disc_module(chnls, 16, bnorm=False),\n",
    "            *disc_module(16, 32), \n",
    "            *disc_module(32, 64),\n",
    "            *disc_module(64, 128)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9502607ac120794b982d5ccb3066dfe72152880354e0dfbf31f715d9cfc03cca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
